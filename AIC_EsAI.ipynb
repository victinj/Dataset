{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AIC - EsAI.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Harta-Tahta-Data/HTD-Compfest-ML/blob/main/AIC_EsAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9EIVPbUM9ZS"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sENpG-rrV7o1"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import operator\n",
        "import re\n",
        "import ast\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import seaborn as sns\n",
        "import urllib\n",
        "from ast import literal_eval\n",
        "from collections import Counter\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dense, Input\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import SimpleRNN, RNN\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lP7ON02EaNiM"
      },
      "source": [
        "# url\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "we-If_OTaRa0"
      },
      "source": [
        "training_dataset = 'https://raw.githubusercontent.com/Harta-Tahta-Data/HTD-Compfest-ML/main/training_dataset.csv'\n",
        "training_dataset_6 = 'https://raw.githubusercontent.com/Harta-Tahta-Data/HTD-Compfest-ML/main/training_dataset_6.csv'\n",
        "id_50 = 'https://raw.githubusercontent.com/Harta-Tahta-Data/HTD-Compfest-ML/main/id_50k%20(1).txt'\n",
        "kata_dataset = 'https://raw.githubusercontent.com/Harta-Tahta-Data/HTD-Compfest-ML/main/110k_kata.csv'\n",
        "kata_baku = 'https://raw.githubusercontent.com/Harta-Tahta-Data/HTD-Compfest-ML/main/kata_baku.csv'\n",
        "kata_tidak_baku = 'https://raw.githubusercontent.com/Harta-Tahta-Data/HTD-Compfest-ML/main/kata_tidak_baku.csv'\n",
        "slang = 'https://raw.githubusercontent.com/louisowen6/NLP_bahasa_resources/master/combined_slang_words.txt'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIucMuMAbyec"
      },
      "source": [
        "# import id_50k.txt\n",
        "files = urllib.request.urlopen(id_50)\n",
        "file_nama = open('id_50k.txt', 'w')\n",
        "for line in files:\n",
        "  decoded_line = line.decode('latin-1')\n",
        "  file_nama.write(decoded_line)\n",
        "file_nama.close()\n",
        "\n",
        "# import kata_tidak_baku\n",
        "kata_tidak_baku = pd.read_csv(kata_tidak_baku)\n",
        "kata_baku = pd.read_csv(kata_baku)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-C0w5UjkbaF"
      },
      "source": [
        "# # # import kata_tidak _baku dan kata_baku \n",
        "# kata_baku = 'https://raw.githubusercontent.com/Harta-Tahta-Data/HTD-Compfest-ML/main/kata_baku.txt'\n",
        "# kata_tidak_baku = 'https://raw.githubusercontent.com/Harta-Tahta-Data/HTD-Compfest-ML/main/kata_tidak_baku.txt'\n",
        "\n",
        "# import ast\n",
        "# from ast import literal_eval\n",
        "# import urllib\n",
        "# from urllib import request\n",
        "# files = urllib.request.urlopen(kata_baku)\n",
        "# file_baku = open('kata_baku.txt', 'w')\n",
        "# for line in files:\n",
        "#   decoded_line = line.decode('latin-1')\n",
        "#   file_baku.write(decoded_line)\n",
        "\n",
        "# file_baku = open('kata_baku.txt', 'r')\n",
        "# kata_baku = {'kata': []}\n",
        "# for line in file_baku:\n",
        "#   if line == 'kata':\n",
        "#     continue\n",
        "#   else:\n",
        "#     kata_baku['kata'].append(line)\n",
        "\n",
        "# files = urllib.request.urlopen(kata_tidak_baku)\n",
        "# file_tidak_baku = open('kata_tidak_baku.txt', 'w')\n",
        "# for line in files:\n",
        "#   decoded_line = line.decode('latin-1')\n",
        "#   file_tidak_baku.write(decoded_line)\n",
        "\n",
        "# file_tidak_baku = open('kata_tidak_baku.txt', 'r')\n",
        "# kata_tidak_baku = {'tidak_baku':[], 'baku':[]}\n",
        "# for line in file_tidak_baku:\n",
        "#   katasiapa = line.split(',')\n",
        "#   if 'tidak_baku' in katasiapa and 'baku' in katasiapa:\n",
        "#     continue\n",
        "#   else:\n",
        "#     kata_tidak_baku['tidak_baku'].append(katasiapa[0])\n",
        "#     kata_tidak_baku['baku'].append(katasiapa[1])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CMTHQArSLX6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "712b417f-edcc-4b6e-9a1e-54f2e77d5a9b"
      },
      "source": [
        "print(len(kata_baku['kata']))\n",
        "print(len(kata_tidak_baku['tidak_baku']))\n",
        "print(len(kata_tidak_baku['baku']))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99670\n",
            "4776\n",
            "4776\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nX4JEm7hM3ei"
      },
      "source": [
        "# Load & preprocess dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8t2V-zBH8htN"
      },
      "source": [
        "pos_tags=pd.read_csv(training_dataset)\n",
        "\n",
        "# fix typo in the dataset\n",
        "pos_tags = pos_tags.replace(\"fw\", \"FW\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_mpk7sd2bBL"
      },
      "source": [
        "# label diurut berdasarkan jumlah POS tag di dataset (descending)\n",
        "labels_sorted = {}\n",
        "\n",
        "for i in range(len(pos_tags)):\n",
        "  if pos_tags[\"pos_tag\"][i] not in labels_sorted:\n",
        "    labels_sorted[pos_tags[\"pos_tag\"][i]] = 1\n",
        "  else:\n",
        "    labels_sorted[pos_tags[\"pos_tag\"][i]] += 1\n",
        "\n",
        "marklist = sorted(labels_sorted.items(), key=operator.itemgetter(1), reverse=True) \n",
        "labels_sorted=dict(marklist)\n",
        "\n",
        "# 23 classes\n",
        "label_list = list(labels_sorted.keys())\n",
        "\n",
        "# id + class\n",
        "labels = pd.DataFrame({\"POS label\": label_list})\n",
        "labels.index += 1"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvVDEWVJgDL1",
        "outputId": "d7d2b09b-98c4-4037-a3f8-301858a50375"
      },
      "source": [
        "label_list"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NN',\n",
              " 'NNP',\n",
              " 'VB',\n",
              " 'Z',\n",
              " 'IN',\n",
              " 'CD',\n",
              " 'SC',\n",
              " 'JJ',\n",
              " 'PRP',\n",
              " 'CC',\n",
              " 'MD',\n",
              " 'PR',\n",
              " 'RB',\n",
              " 'FW',\n",
              " 'SYM',\n",
              " 'NEG',\n",
              " 'NND',\n",
              " 'OD',\n",
              " 'DT',\n",
              " 'X',\n",
              " 'WH',\n",
              " 'RP',\n",
              " 'UH']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtN-ZHfkPblM"
      },
      "source": [
        "# load dataset\n",
        "sentence_pos_tags=pd.read_csv(training_dataset_6)\n",
        "\n",
        "# set X and Y\n",
        "X = [literal_eval(i) for i in sentence_pos_tags[\"sentences\"]]\n",
        "Y = [literal_eval(i) for i in sentence_pos_tags[\"pos_tag\"]]\n",
        "X_sentences = X.copy()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WR6EumQXFsU"
      },
      "source": [
        "# Encoding X and Y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHQGsXVrW2Jj"
      },
      "source": [
        "word_tokenizer = Tokenizer()\n",
        "word_tokenizer.fit_on_texts(X)\n",
        "X_encoded = word_tokenizer.texts_to_sequences(X)\n",
        "\n",
        "tag_tokenizer = Tokenizer()\n",
        "tag_tokenizer.fit_on_texts(Y)\n",
        "Y_encoded = tag_tokenizer.texts_to_sequences(Y)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mibBRK0gXDa3"
      },
      "source": [
        "\n",
        "lengths = [len(seq) for seq in X_encoded]\n",
        "\n",
        "MAX_SEQ_LENGTH = max(lengths)  # sequences with length larger than 105 will be truncated\n",
        "\n",
        "X_padded = pad_sequences(X_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")\n",
        "Y_padded = pad_sequences(Y_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")\n",
        "\n",
        "# assign padded sequences to X and Y\n",
        "X, Y = X_padded, Y_padded"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfWWlxtShezs"
      },
      "source": [
        "# Word embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVD-QNguIT2d"
      },
      "source": [
        "# define model\n",
        "model = Word2Vec(sentences=X_sentences)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHANYwHA0wGQ"
      },
      "source": [
        "# hyperparameters\n",
        "EMBEDDING_SIZE  = model.wv.vector_size                  # 100\n",
        "VOCABULARY_SIZE = len(word_tokenizer.word_index) + 1    # 17124"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dZiyLPnhjrY"
      },
      "source": [
        "# create an empty embedding matix\n",
        "embedding_weights = np.zeros((VOCABULARY_SIZE, EMBEDDING_SIZE))\n",
        "\n",
        "# create a word to index dictionary mapping\n",
        "word2id = word_tokenizer.word_index\n",
        "\n",
        "# copy vectors from word2vec model to the words present in corpus\n",
        "for word, index in word2id.items():\n",
        "    try:\n",
        "        embedding_weights[index, :] = model.wv[word]\n",
        "    except KeyError:\n",
        "        pass\n",
        "\n",
        "Y = to_categorical(Y)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHZdZ3CD9PhP"
      },
      "source": [
        "# save\n",
        "model.wv.save_word2vec_format(\"custom_word_embedding_id.txt\")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xa3vesnX2AtA"
      },
      "source": [
        "# Split data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qn5iPKMUIeZZ"
      },
      "source": [
        "TEST_SIZE = 0.15\n",
        "VALID_SIZE = 0.15"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84-tRctO1iGo"
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=TEST_SIZE, random_state=4)\n",
        "X_train, X_validation, Y_train, Y_validation = train_test_split(X_train, Y_train, test_size=VALID_SIZE, random_state=4)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdKyNKdp2a9p"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr5m5N3dLda1"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbxnubE3LbZ5"
      },
      "source": [
        "# plot training\n",
        "def plot_training(training_model):\n",
        "    plt.plot(training_model.history[\"acc\"])\n",
        "    plt.plot(training_model.history[\"val_acc\"])\n",
        "    plt.title(\"model accuracy\")\n",
        "    plt.ylabel(\"accuracy\")\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.legend([\"train\", \"test\"], loc=\"lower right\")\n",
        "    plt.show()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIaCmBDKOTd6"
      },
      "source": [
        "# preprocess kalimat\n",
        "def split_kalimat(kalimat):\n",
        "    kalimat_baru = []\n",
        "    kalimat = re.split(r\"(\\W+)\", kalimat)\n",
        "\n",
        "    while \"\" in kalimat:\n",
        "        kalimat.remove(\"\")\n",
        "\n",
        "    while \" \" in kalimat:\n",
        "        kalimat.remove(\" \")\n",
        "\n",
        "    kalimat = [x.strip(\" \") for x in kalimat]\n",
        "\n",
        "    for x in kalimat:\n",
        "        x = x.split(\" \")\n",
        "        for i in x:\n",
        "            kalimat_baru.append(i)\n",
        "\n",
        "    return kalimat_baru"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6-hJgV1FxOw"
      },
      "source": [
        "## RNN model (w/ weights from the word embedding)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdoToJFYAKeo"
      },
      "source": [
        "# hyperparameters\n",
        "EMBEDDING_SIZE  = model.wv.vector_size                  # 100\n",
        "VOCABULARY_SIZE = len(word_tokenizer.word_index) + 1    # 17124\n",
        "NUM_CLASSES = Y.shape[2]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERNFxDVq_UGL"
      },
      "source": [
        "rnn_model = Sequential()\n",
        "\n",
        "rnn_model.add(Embedding(input_dim=VOCABULARY_SIZE,\n",
        "                        output_dim=EMBEDDING_SIZE,\n",
        "                        input_length=MAX_SEQ_LENGTH,\n",
        "                        weights=[embedding_weights],\n",
        "                        trainable=True))\n",
        "\n",
        "rnn_model.add(SimpleRNN(64, return_sequences=True))\n",
        "\n",
        "rnn_model.add(TimeDistributed(Dense(NUM_CLASSES, activation=\"softmax\")))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRv4rSbOCPaW"
      },
      "source": [
        "rnn_model.compile(loss=\"categorical_crossentropy\",\n",
        "                   optimizer=\"adam\",\n",
        "                   metrics=[\"acc\"])"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pT9k_wUUCEUK",
        "outputId": "f70c13c3-9c06-4e12-ba12-60d499fddb93"
      },
      "source": [
        "rnn_training = rnn_model.fit(X_train,\n",
        "                             Y_train,\n",
        "                             batch_size=128,\n",
        "                             epochs=10,\n",
        "                             validation_data=(X_validation, Y_validation))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "60/60 [==============================] - 7s 99ms/step - loss: 1.1314 - acc: 0.7757 - val_loss: 0.5327 - val_acc: 0.8651\n",
            "Epoch 2/10\n",
            "60/60 [==============================] - 6s 95ms/step - loss: 0.4189 - acc: 0.8994 - val_loss: 0.3302 - val_acc: 0.9271\n",
            "Epoch 3/10\n",
            "60/60 [==============================] - 6s 94ms/step - loss: 0.2478 - acc: 0.9494 - val_loss: 0.1942 - val_acc: 0.9616\n",
            "Epoch 4/10\n",
            "60/60 [==============================] - 6s 93ms/step - loss: 0.1433 - acc: 0.9731 - val_loss: 0.1230 - val_acc: 0.9753\n",
            "Epoch 5/10\n",
            "60/60 [==============================] - 6s 94ms/step - loss: 0.0911 - acc: 0.9820 - val_loss: 0.0904 - val_acc: 0.9803\n",
            "Epoch 6/10\n",
            "60/60 [==============================] - 6s 94ms/step - loss: 0.0655 - acc: 0.9863 - val_loss: 0.0739 - val_acc: 0.9830\n",
            "Epoch 7/10\n",
            "60/60 [==============================] - 6s 95ms/step - loss: 0.0515 - acc: 0.9888 - val_loss: 0.0647 - val_acc: 0.9843\n",
            "Epoch 8/10\n",
            "60/60 [==============================] - 6s 94ms/step - loss: 0.0427 - acc: 0.9904 - val_loss: 0.0588 - val_acc: 0.9851\n",
            "Epoch 9/10\n",
            "60/60 [==============================] - 6s 94ms/step - loss: 0.0367 - acc: 0.9914 - val_loss: 0.0550 - val_acc: 0.9856\n",
            "Epoch 10/10\n",
            "60/60 [==============================] - 6s 94ms/step - loss: 0.0324 - acc: 0.9921 - val_loss: 0.0524 - val_acc: 0.9862\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bs8LwpF4F2SF",
        "outputId": "3b84105d-c798-48f0-ed46-7a048a3fc555"
      },
      "source": [
        "# evaluate accuracy on test set\n",
        "loss, accuracy = rnn_model.evaluate(X_test, Y_test, verbose=1)\n",
        "print(\"Loss: {0},\\nAccuracy: {1}\".format(loss, accuracy))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50/50 [==============================] - 1s 11ms/step - loss: 0.0513 - acc: 0.9862\n",
            "Loss: 0.051318634301424026,\n",
            "Accuracy: 0.9862321019172668\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNuaSJc1K36Z"
      },
      "source": [
        "rnn_model.save('model_v6.h5')"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OegHY5hNdpV"
      },
      "source": [
        "import pickle\n",
        "\n",
        "# saving\n",
        "with open('tokenizer_v6.pickle', 'wb') as handle:\n",
        "    pickle.dump(word_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyYziLZ9UakM"
      },
      "source": [
        "# POS tagger"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a90pnE9FaS2t"
      },
      "source": [
        "# prediksi POS tag kalimat\n",
        "def prediksi_pos(kalimat):\n",
        "    kalimat_encoded = word_tokenizer.texts_to_sequences([kalimat])\n",
        "    kalimat_padded = pad_sequences(kalimat_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")\n",
        "\n",
        "    pred_kalimat = rnn_model.predict(kalimat_padded[0])\n",
        "\n",
        "    result = []\n",
        "    for i in range(len(pred_kalimat)):\n",
        "        id = np.argmax(pred_kalimat[i][0])\n",
        "        if id != 0:\n",
        "            result.append(label_list[id-1])\n",
        "\n",
        "    # unknown words detected\n",
        "    if len(result) != len(kalimat):\n",
        "        for id, i in enumerate(kalimat):\n",
        "            word_encoded = word_tokenizer.texts_to_sequences([[i]])\n",
        "            word_padded = pad_sequences(word_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")\n",
        "            if np.count_nonzero(word_padded) == 0:\n",
        "                result.insert(id, \"X\")\n",
        "    \n",
        "    return result"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEgntwsYRtvq",
        "outputId": "cd033028-67f9-4b59-adee-e3e86b646f4b"
      },
      "source": [
        "print(MAX_SEQ_LENGTH)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tfYAMBQB6Hp"
      },
      "source": [
        "# Typo checker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHodNcjYBPM1"
      },
      "source": [
        "## Dataset: id_50k.txt\n",
        "[Sumber](https://github.com/hermitdave/FrequencyWords/blob/master/content/2018/id/id_50k.txt)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTO2QAv_FtoB"
      },
      "source": [
        "import ast\n",
        "from ast import literal_eval\n",
        "# open/import slang words dataset\n",
        "#slang = open(\"combined_slang_words.txt\", \"r\")\n",
        "#contents = slang.read()\n",
        "#slang_dict = ast.literal_eval(contents)\n",
        "#slang.close()\n",
        "files = urllib.request.urlopen(slang)\n",
        "file_slang = open('combined_slang_words.txt', 'w')\n",
        "for line in files:\n",
        "  decoded_line = line.decode('latin-1')\n",
        "  file_slang.write(decoded_line)\n",
        "\n",
        "file_slang = open('combined_slang_words.txt', 'r')\n",
        "contents = file_slang.read()\n",
        "slang_dict = ast.literal_eval(contents)\n",
        "file_slang.close()"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIVuod8-4QbU"
      },
      "source": [
        "# http://norvig.com/spell-correct.html\n",
        "\n",
        "def words(text):\n",
        "    words = []\n",
        "    lines = text.split(\"\\n\")\n",
        "    for i, line in enumerate(lines):\n",
        "        word = lines[i].split(\" \")[0]\n",
        "        words.append(word)\n",
        "    return words\n",
        "\n",
        "WORDS = words(open('id_50k.txt').read())\n",
        "\n",
        "def P(word): \n",
        "    \"Probability of `word`.\"\n",
        "    word_counts = []\n",
        "    lines = open('id_50k.txt').read().split(\"\\n\")\n",
        "    for i, line in enumerate(lines):\n",
        "        word_count = lines[i].split(\" \")[1]\n",
        "        word_counts.append(word_count)\n",
        "    word_prob = word_counts[WORDS.index(word)]\n",
        "    return int(word_prob)\n",
        "\n",
        "def correction(word): \n",
        "    \"Most probable spelling correction for word.\"\n",
        "    try:\n",
        "        if word in slang_dict:\n",
        "            max_word = slang_dict[word]\n",
        "        else:\n",
        "            max_word = max(candidates(word), key=P)\n",
        "    except ValueError:\n",
        "        max_word = word\n",
        "    return max_word\n",
        "\n",
        "def candidates(word): \n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    candidates = (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "    new_candidates = candidates.copy()\n",
        "    word_counts = []\n",
        "    lines = open('id_50k.txt').read().split(\"\\n\")\n",
        "    for i, line in enumerate(lines):\n",
        "        word_count = lines[i].split(\" \")[0]\n",
        "        word_counts.append(word_count)\n",
        "\n",
        "    for i in candidates:\n",
        "        # csv\n",
        "\n",
        "        if i in kata_tidak_baku['tidak_baku'].values:\n",
        "            new_candidates.remove(i)\n",
        "            new_word = kata_tidak_baku['baku'][kata_tidak_baku.loc[kata_tidak_baku['tidak_baku'] == i].index[0]]\n",
        "            # print(new_word, type(new_word))\n",
        "            if new_word in word_counts:\n",
        "                new_candidates.add(new_word)\n",
        "        elif i not in kata_baku['kata'].values:\n",
        "            new_candidates.remove(i)\n",
        "        \n",
        "        #txt\n",
        "\n",
        "        # if i in kata_tidak_baku['tidak_baku']:\n",
        "        #     new_candidates.remove(i)\n",
        "        #     new_word = kata_tidak_baku['baku'][kata_tidak_baku['tidak_baku'].index(i)]\n",
        "        #     print(new_word, type(new_word))\n",
        "        #     if new_word in word_counts:\n",
        "        #       new_candidates.add(new_word)\n",
        "        # elif i not in kata_baku['kata']:\n",
        "        #     new_candidates.remove(i)\n",
        "\n",
        "    return new_candidates\n",
        "\n",
        "def known(words): \n",
        "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "\n",
        "def edits1(word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word): \n",
        "    \"All edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rg5R3XzSJJVd",
        "outputId": "3848e2c3-961f-402a-c4be-2afa2135650a"
      },
      "source": [
        "candidates('apotik')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'apotek'}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "3dC4kNMjqP7F",
        "outputId": "29a2a746-3691-49b2-a015-490113eb42fc"
      },
      "source": [
        "if 'apotik' in kata_tidak_baku['tidak_baku']:\n",
        "  print(True)\n",
        "kata_tidak_baku.loc[kata_tidak_baku['tidak_baku'] == 'apotik']"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tidak_baku</th>\n",
              "      <th>baku</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>apotik</td>\n",
              "      <td>apotek</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   tidak_baku    baku\n",
              "97     apotik  apotek"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxqV-QjpNQbv"
      },
      "source": [
        "# Grammar checker (rules)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZN89Oy8Wb8yG"
      },
      "source": [
        "## Sebelum POS tag"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxorhBQEapo_"
      },
      "source": [
        "di = ['dia', 'diabetes', 'diad', 'diadem', 'diafon', 'diaforetik', 'diafragma', 'diagenesis', 'diagnosis', 'diagnostik', 'diagometer', 'diagonal', 'diagram', 'diaken', 'diakon', 'diakones', 'diakonia', 'diakritik', 'diakronis', 'dialek', 'dialektal', 'dialektik', 'dialektika', 'dialektis', 'dialektologi', 'dialinguistik', 'dialisis', 'dialog', 'dialogis', 'diam', 'diamagnetisme', 'diameter', 'diametral', 'diamorf', 'dian', 'diang', 'diaper', 'diapositif', 'diar', 'diare', 'dias', 'diasistem', 'diaspora', 'diastase', 'diastole', 'diat', 'diaterman', 'diatermi', 'diatermik', 'diatesis', 'diatipe', 'diatom', 'diatomit', 'diatonik', 'diatopik', 'diayah', 'dibasa', 'didaktik', 'didaktikus', 'didaktis', 'didih', 'didik', 'didis', 'didong', 'dielektrik', 'diensefalon', 'dies natalis', 'diesel', 'diet', 'dietetika', 'difabel', 'diferensial', 'diferensiasi', 'difluens', 'difluensi', 'difraksi', 'difteri', 'diftong', 'difusi', 'digdaya', 'digenesis', 'digestif', 'digit', 'digital', 'digitalin', 'digitalis', 'digitalisasi', 'diglosia', 'digraf', 'digresi', 'Digul', 'dihedral', 'dihidroksil', 'dik', 'dikara', 'dikau', 'dikit', 'diklorida', 'dikotil', 'dikotomi', 'dikroisme', 'dikromat', 'dikromatik', 'diksa', 'diksi', 'diktat', 'diktator', 'diktatorial', 'diktatoris', 'dikte', 'diktum', 'dil', 'dila', 'dilak', 'dilam', 'dilasi', 'dilatasi', 'dilatometer', 'dilema', 'dilematik', 'diler', 'diletan', 'diluvium', 'dim', 'dimensi', 'dimer', 'diminutif', 'dimorfik', 'dimorfisme', 'din', 'dina', 'dinamik', 'dinamika', 'dinamis', 'dinamisator', 'dinamisme', 'dinamit', 'dinamo', 'dinamometer', 'dinar', 'dinasti', 'dinding', 'dingin', 'dingkis', 'dingkit', 'dingklang', 'dingklik', 'dingo', 'dini', 'diniah', 'dinosaurus', 'diode', 'dioesis', 'dioksida', 'dioksin', 'diopsida', 'dioptase', 'dioptri', 'diorama', 'diorit', 'dioses', 'dipan', 'diplo', 'diploid', 'diploma', 'diplomasi', 'diplomat', 'diplomatik', 'diplomatis', 'dipsomania', 'diptera', 'diptotos', 'dirah', 'diraja', 'direk', 'direksi', 'direktorat', 'direktorium', 'direktris', 'direktur', 'dirgahayu', 'dirgantara', 'dirham', 'diri', 'dirigen', 'diris', 'dirus', 'disagio', 'disakarida', 'disastria', 'disbursemen', 'disdrometer', 'disekuilibrium', 'disensus', 'disentri', 'disertasi', 'disfonia', 'disfungsi', 'disharmoni', 'disiden', 'disilabik', 'disimilasi', 'disinfektan', 'disinformasi', 'disinsentif', 'disintegrasi', 'disiplin', 'disjoki', 'disjungsi', 'disjungtif', 'diska', 'disket', 'diskiasis', 'disklimaks', 'disko', 'diskoid', 'diskon', 'diskontinu', 'diskontinuitas', 'diskonto', 'diskordans', 'diskorobik', 'diskotek', 'diskredit', 'diskrepansi', 'diskresi', 'diskriminasi', 'diskriminatif', 'diskriminator', 'diskualifikasi', 'diskulpasi', 'diskursif', 'diskus', 'diskusi', 'dislalia', 'disleksia', 'dislokasi', 'dismembrasio', 'dismenorea', 'dismutasi', 'disolventia', 'disonansi', 'disoperasi', 'disorder', 'disorganisasi', 'disorientasi', 'disosiasi', 'dispareunia', 'disparitas', 'dispensasi', 'dispenser', 'dispepsia', 'dispersal', 'dispersi', 'disposisi', 'disposotio', 'disprosium', 'disrupsi', 'distabilitas', 'distal', 'distansi', 'distikiasis', 'distikon', 'distilasi', 'distilator', 'distingsi', 'distingtif', 'distoma', 'distorsi', 'distosia', 'distribusi', 'distributor', 'distrik', 'disuasi', 'disuria', 'dito', 'ditransitif', 'diuresis', 'diuretik', 'diurnal', 'divergen', 'divergensi', 'diversifikasi', 'diversitas', 'divestasi', 'dividen', 'divisi']\n",
        "ke = ['kebab', 'kebabal', 'kebah', 'kebaji', 'kebal', 'kebam', 'kebas', 'kebat', 'kebaya', 'kebayan', 'kebel', 'kebelet', 'kebembem', 'kebin', 'kebiri', 'keblangsak', 'keblinger', 'kebuk', 'kebul', 'kebuli', 'kebun', 'kebur', 'kebut', 'kebyar', 'kecai', 'kecak', 'kecak pinggang', 'kecalingan', 'kecam', 'kecambah', 'kecamuk', 'kecandan', 'kecap', 'kecapi', 'kecar', 'kece', 'kecebong', 'kecek', 'kecele', 'keceng', 'kecepek', 'kecer', 'kecewa', 'keci', 'keciak', 'kecibak', 'kecibeling', 'kecik', 'kecil', 'kecil hati', 'kecimik', 'kecimpring', 'kecimpung', 'kecimus', 'kecipak', 'kecipir', 'kecipuk', 'kecit', 'keciut', 'kecoak', 'kecoh', 'kecombrang', 'kecong', 'kecrek', 'kecu', 'kecuali', 'kecubung', 'kecumik', 'kecup', 'kecut', 'kedabu', 'kedadak', 'kedah', 'kedai', 'kedal', 'kedaluwarsa', 'kedam', 'kedang', 'kedangkai', 'kedangkan', 'kedangsa', 'kedap', 'kedasih', 'kedau', 'kedaung', 'kedayan', 'kedebong', 'kedek', 'kedekai', 'kedeki', 'kedekut', 'kedelai', 'kedele', 'kedemplung', 'kedempung', 'kedengkang', 'keder', 'kedera', 'kederang', 'kedewaga', 'kedi', 'kedidi', 'kedik', 'kedikit', 'kedip', 'kedok', 'kedondong', 'kedongkok', 'kedot', 'kedua', 'keduanya', 'keduduk', 'keduk', 'kedul', 'kedumung', 'kedung', 'kedut', 'keferdom', 'kehel', 'keibodan', 'kejai', 'kejam', 'kejamas', 'kejan', 'kejang', 'kejap', 'kejar', 'kejat', 'kejawen', 'kejen', 'kejer', 'keji', 'kejip', 'Kejora', 'keju', 'kejuju', 'kejur', 'kejut', 'kek', 'kekah', 'kekal', 'kekam', 'kekandi', 'kekang', 'kekapas', 'kekar', 'kekara', 'kekas', 'kekat', 'kekau', 'kekawin', 'kekeba', 'kekebik', 'kekeh', 'kekek', 'kekel', 'kekemben', 'kekep', 'keker', 'keki', 'kekitir', 'kekok', 'kekol', 'kelab', 'kelabak', 'kelabang', 'kelabat', 'kelabau', 'kelabu', 'keladak', 'keladan', 'keladau', 'keladi', 'kelah', 'kelahi', 'kelai', 'kelak', 'kelakah', 'kelakanji', 'kelakar', 'kelalang', 'kelam', 'kelambai', 'kelambir', 'kelambit', 'kelambu', 'kelambur', 'kelamin', 'kelamkari', 'kelana', 'kelang', 'kelangkan', 'kelangkang', 'kelanjar', 'kelantang', 'kelapa', 'kelar', 'kelara', 'kelarah', 'kelarai', 'kelaras', 'kelari', 'kelas', 'kelasa', 'kelasah', 'kelasak', 'kelasi', 'kelat', 'kelati', 'kelawan', 'kelayan', 'kelayang', 'kelayu', 'kelder', 'kelebat', 'kelebek', 'kelebet', 'kelebu', 'kelebuk', 'kelebut', 'keledai', 'keledang', 'keledar', 'keledek', 'kelejat', 'kelek', 'kelekap', 'kelekatu', 'kelelap', 'kelelawar', 'kelelesa', 'kelelot', 'kelemayar', 'kelemayuh', 'kelembahang', 'kelembak', 'kelemban', 'kelembuai', 'kelempai', 'kelemping', 'kelemur', 'kelendara', 'keleneng', 'kelengar', 'kelenggara', 'kelengkeng', 'kelengkiak', 'kelening', 'kelenjar', 'kelentang', 'kelenteng', 'kelentik', 'kelenting', 'kelentit', 'kelentong', 'kelentung', 'kelenung', 'kelenyit', 'kelep', 'kelepai', 'kelepak', 'kelepat', 'kelepek', 'kelepet', 'kelepik', 'kelepir', 'kelepit', 'kelepuk', 'kelepur', 'keler', 'kelereng', 'kelesa', 'kelesah', 'keleseh peseh', 'kelesek', 'kelesot', 'keletah', 'keletak', 'keletang', 'keletik', 'keletuk', 'keletung', 'kelewang', 'keli', 'kelian', 'keliar', 'kelibang', 'kelibat', 'kelicap', 'kelici', 'kelih', 'kelijak', 'kelik', 'kelika', 'kelikat', 'keliki', 'kelikih', 'kelikik', 'kelikir', 'keliling', 'kelilip', 'kelim', 'kelimpanan', 'kelimpungan', 'kelimun', 'kelimut', 'kelinci', 'kelincir', 'kelindan', 'kelingking', 'kelingsir', 'kelining', 'kelinjat', 'kelintang', 'kelintar', 'kelinting', 'kelip', 'kelipat', 'kelir', 'keliru', 'kelis', 'kelisera', 'kelit', 'keliti', 'kelitik', 'keliwon', 'kelobot', 'kelobotisme', 'kelocak', 'keloceh', 'kelodan', 'keloelektrovolt', 'keloid', 'kelojot', 'kelok', 'kelokak', 'kelola', 'kelolong', 'kelom', 'kelombeng', 'kelompang', 'kelompen', 'kelompok', 'kelon', 'keloneng', 'kelonet', 'kelong', 'kelongkong', 'kelongsong', 'kelontang', 'kelontong', 'kelonyo', 'kelop', 'kelopak', 'kelor', 'kelorak', 'kelos', 'kelosok', 'kelotok', 'keloyak', 'keloyang', 'keloyor', 'kelp', 'kelu', 'kelua', 'keluai', 'keluak', 'keluan', 'keluang', 'keluangsa', 'keluar', 'keluarga', 'kelubak', 'kelubi', 'keluburan', 'keluh', 'kelui', 'keluih', 'keluk', 'kelukup', 'kelukur', 'keluli', 'kelulu', 'kelulus', 'kelulut', 'kelumit', 'kelumpang', 'kelumun', 'kelun', 'keluna', 'kelunak', 'kelung', 'kelupas', 'kelupur', 'keluron', 'kelurut', 'kelus', 'kelut', 'kelutum', 'keluwung', 'keluyuk', 'keluyur', 'kemah', 'kemal', 'kemala', 'kemam', 'kemamang', 'kemang', 'kemangi', 'kemarau', 'kemari', 'kemarin', 'kemaruk', 'kemas', 'kemat', 'kematu', 'kematus', 'kemayu', 'kembal', 'kembali', 'kemban', 'kembang', 'kembang biak', 'kembar', 'kembatu', 'kembayat', 'kembeng', 'kembera', 'kembili', 'kemboja', 'kembol', 'kembu', 'kembuk', 'kembung', 'kembur', 'kembut', 'kemeja', 'kemejan', 'kemekmek', 'kemelut', 'kemenakan', 'kemendalam', 'kemendang', 'kemendur', 'kementam', 'kemenyan', 'kemerakan', 'kemesu', 'kemi', 'kemih', 'kemik', 'kemilap', 'kemiluminesens', 'kemiri', 'kemit', 'kemlaka', 'kemlandingan', 'kemoceng', 'kemokinesis', 'kemon', 'kemopsikiatri', 'kemoterapi', 'kempa', 'kempal', 'kempang', 'kempas', 'kempek', 'kempes', 'kempetai', 'kempis', 'kempit', 'kemplang', 'kempot', 'kempu', 'kempuh', 'kempul', 'kempunan', 'kempung', 'kemput', 'kempyang', 'kemu', 'kemudi', 'kemudian', 'kemukus', 'kemul', 'kemumu', 'kemuncup', 'kemung', 'kemungkus', 'kemuning', 'kemunting', 'kemurgi', 'kemut', 'kemutul', 'ken', 'kena', 'kenaf', 'kenal', 'kenan', 'kenang', 'kenanga', 'kenap', 'kenapa', 'kenapang', 'kenari', 'kenas', 'kencan', 'kencana', 'kencang', 'kencar', 'kenceng', 'kencing', 'kencit', 'kencong', 'kencreng', 'kencung', 'kencur', 'kendaga', 'kendal', 'kendala', 'kendali', 'kendana', 'kendang', 'kendara', 'kendati', 'kendayakan', 'kendeka', 'kenderi', 'kendi', 'kendil', 'kendit', 'kendo', 'kendong', 'kenduduk', 'kendung', 'kendur', 'kenduri', 'kenek', 'keneker', 'kenem', 'kenematik', 'kenes', 'keng', 'kengkang', 'kengkeng', 'kenidai', 'kenikir', 'kening', 'kenohong', 'kenong', 'kenop', 'kensel', 'kental', 'kentang', 'kentar', 'kentara', 'kenteng', 'kentrung', 'kentung', 'kentut', 'kenur', 'kenya', 'kenyal', 'kenyam', 'kenyang', 'kenyat', 'kenyi', 'kenyih', 'kenyir', 'kenyit', 'kenyut', 'keok', 'keong', 'kep', 'kepada', 'kepah', 'kepai', 'kepak', 'kepal', 'kepala', 'kepalang', 'kepam', 'kepang', 'kepar', 'keparat', 'kepayang', 'kepecong', 'kepek', 'kepel', 'kepencong', 'kepeng', 'keper', 'keperancak', 'kepet', 'kepetang', 'kepialu', 'kepiat', 'kepik', 'kepil', 'kepincut', 'kepinding', 'keping', 'kepingin', 'kepinis', 'kepinjal', 'kepiri', 'kepis', 'kepit', 'kepiting', 'keplak', 'kepleset', 'keplok', 'kepodang', 'kepoh', 'kepol', 'keponakan', 'kepot', 'keprak', 'keprek', 'kepret', 'kepris', 'kepruk', 'kepuh', 'kepuk', 'kepul', 'kepulaga', 'kepundan', 'kepundung', 'kepung', 'kepurun', 'keputren', 'kepuyuk', 'kera', 'kerabang', 'kerabat', 'kerabik', 'kerabu', 'keracak', 'keracap', 'keraeng', 'kerah', 'kerahi', 'kerai', 'kerajang', 'kerajat', 'kerak', 'kerakah', 'kerakal', 'kerakap', 'kerakeling', 'keram', 'kerama', 'keraman', 'keramas', 'keramat', 'keramba', 'kerambil', 'kerambit', 'keramboja', 'keramik', 'keramikus', 'kerampagi', 'kerampang', 'keramunting', 'keran', 'kerancang', 'keranda', 'kerang', 'kerangas', 'kerangkai', 'kerangkeng', 'kerani', 'keranjang', 'keranjat', 'keranji', 'keranta', 'kerantong', 'kerap', 'kerapu', 'keras', 'kerasan', 'kerat', 'keratabasa', 'keratin', 'keratitis', 'keratoelastin', 'keraton', 'kerau', 'kerawai', 'kerawak', 'kerawang', 'kerawat', 'kerawit', 'kerbang', 'kerbat', 'kerbau', 'kerbuk', 'kercing', 'kercit', 'kercut', 'kerdak', 'kerdam', 'kerdil', 'kerdom', 'kere', 'kerebok', 'kereceng', 'kerecik', 'keredak', 'keredep', 'keredok', 'keredong', 'kerek', 'kereket', 'kerekot', 'kerekut', 'keremi', 'keremot', 'kerempeng', 'kerempung', 'keremus', 'keren', 'kerencang', 'kerencung', 'kerendang', 'kereneng', 'kereng', 'kerengga', 'kerengkam', 'kerentam', 'kerentang', 'kerenting', 'kerenyam', 'kerenyot', 'kerepek', 'kerepes', 'kerepot', 'kerepyak', 'kerese pese', 'keresek', 'kereseng', 'keresot', 'kereta', 'kereta api', 'keretan', 'keretek', 'keretot', 'keretut', 'kereweng', 'keri', 'keriap', 'kerias', 'keriau', 'kerical', 'kericau', 'keridas', 'keridik', 'kerih', 'kerikal', 'kerikam', 'kerikil', 'kerikit', 'kerimut', 'kerinan', 'kerincing', 'kerinding', 'kering', 'keringat', 'keriningan', 'kerinjal', 'kerinjang', 'kerinjing', 'kerintil', 'kerinting', 'kerip', 'keripik', 'keriput', 'keris', 'kerisi', 'kerisik', 'kerising', 'kerisut', 'kerit', 'keritik', 'keriting', 'keriuk', 'keriut', 'kerja', 'kerjang', 'kerjantara', 'kerjap', 'kerkah', 'kerkak', 'kerkap', 'kerkau', 'kerkop', 'kerkup', 'kerlap', 'kerling', 'kerlip', 'kermak', 'kermanici', 'kermi', 'kernai', 'kerneli', 'kernet', 'kernu', 'kernyau', 'kernyih', 'kernying', 'kernyit', 'kernyut', 'kero', 'kerobak', 'kerobat', 'kerobek', 'keroco', 'kerocok', 'kerogen', 'keroh', 'kerok', 'kerokot', 'keromong', 'keron', 'keroncang', 'keroncong', 'keroncor', 'kerong', 'kerong kerang', 'kerongsang', 'kerontang', 'kerop', 'keropak', 'keropeng', 'keropok', 'keropos', 'kerosak', 'kerosek', 'kerosi', 'kerosin', 'kerosok', 'kerosong', 'kerot', 'kerotak', 'kerotot', 'keroyok', 'kerpai', 'kerpak', 'kerpas', 'kerpubesi', 'kerpuk', 'kerpus', 'kers', 'kersai', 'kersak', 'kersang', 'kersani', 'kersen', 'kersik', 'kersuk', 'kertaaji', 'kertah', 'kertak', 'kertang', 'kertap', 'kertas', 'kertau', 'kertuk', 'kertus', 'keruan', 'kerubin', 'kerubung', 'kerubut', 'kerucil', 'kerucut', 'kerudung', 'keruh', 'keruh bumi', 'keruit', 'keruk', 'kerukut', 'kerul', 'keruma', 'kerumit', 'kerumuk', 'kerumun', 'kerumus', 'kerun', 'kerung', 'kerunkel', 'kerunting', 'keruntung', 'kerunyut', 'kerup', 'kerupuk', 'kerut', 'kerutak', 'kerutup', 'keruyuk', 'kes', 'kesah', 'kesak', 'kesal', 'kesam', 'kesambet', 'kesambi', 'kesan', 'kesang', 'kesat', 'kesatria', 'kesek', 'kesel', 'keseleo', 'kesemek', 'keseran', 'keseser', 'keset', 'kesi', 'kesiap', 'kesik', 'kesima', 'kesimbukan', 'kesip', 'kesiur', 'keskul', 'kesomplok', 'kesongo', 'kesot', 'kesrakat', 'kesting', 'kesturi', 'kesuma', 'kesumat', 'kesumba', 'kesup', 'kesusu', 'kesut', 'keta', 'ketaban', 'ketai', 'ketak', 'ketakong', 'ketal', 'ketam', 'ketambak', 'ketampi', 'ketan', 'ketang', 'ketap', 'ketapak', 'ketapang', 'ketapek', 'ketar', 'ketarap', 'ketat', 'ketaton', 'ketaya', 'ketayap', 'ketegar', 'ketek', 'ketel', 'ketela', 'ketemu', 'ketena', 'keteng', 'ketepel', 'ketepeng', 'keter', 'ketes', 'keteter', 'ketgat', 'keti', 'ketiak', 'ketial', 'ketiap', 'ketiau', 'ketiban', 'ketiding', 'ketik', 'ketika', 'ketil', 'ketilang', 'ketimbis', 'ketimbul', 'keting', 'ketinjau', 'ketinting', 'ketip', 'ketiplak', 'ketipung', 'ketirah', 'ketis', 'ketitir', 'ketogenesis', 'ketok', 'ketola', 'ketombe', 'ketonemia', 'ketonggeng', 'ketonuria', 'ketopong', 'ketoprak', 'ketosa', 'ketrek', 'ketu', 'ketua', 'ketuat', 'ketuban', 'ketuir', 'ketuk', 'ketul', 'ketumbar', 'ketumbi', 'ketumbit', 'ketumbu', 'ketumpang', 'ketun', 'ketungging', 'ketup', 'ketupa', 'ketupat', 'ketupuk', 'ketur', 'ketus', 'kevin']"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4NymRypESND"
      },
      "source": [
        "# grammar rules before applying POS tags\n",
        "grammar = [\n",
        "\t{\n",
        "\t\t\"pattern\":r\"(?<!\\w)di\\w+\",\n",
        "\t\t\"pesan\":\"'di' harus pisah dengan kata benda/tempat.\",\n",
        "        \"koreksi\":\"\",  # \n",
        "        \"contoh\":\"di kantor\",\n",
        "        \"status\":\"error1\",\n",
        "        \"pengecualian\":di,\n",
        "        \"err_id\":1\n",
        "\t},\n",
        "    {\n",
        "\t\t\"pattern\":r\"^(ke\\w+)\",\n",
        "\t\t\"pesan\":\"'ke' harus pisah dengan kata yang mengikutinya.\",\n",
        "        \"koreksi\":\"\",  # \n",
        "        \"contoh\":\"ke sana\",\n",
        "        \"status\":\"error1\",\n",
        "        \"pengecualian\":ke,\n",
        "        \"err_id\":2\n",
        "\t},\n",
        "    {\n",
        "\t\t\"pattern\":r\"\\w+pun\",\n",
        "\t\t\"pesan\":\"Partikel 'pun' harus dipisah dengan kata sebelumnya.\",\n",
        "        \"koreksi\":\"\",  # \n",
        "        \"contoh\":\"makanan pun\",\n",
        "        \"status\":\"error1\",\n",
        "        \"pengecualian\":[\"meskipun\", \"adapun\", \"bagaimanapun\", \"walaupun\", \"ampun\"],\n",
        "        \"err_id\":3\n",
        "\t}\n",
        "]"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49LRi-z_GAgZ"
      },
      "source": [
        "def word_checker(testing):\n",
        "    # iterate through each word in a sentence\n",
        "    for id, sentence in enumerate(testing):\n",
        "        # iterate through possible errors\n",
        "        for err in grammar:\n",
        "            found = re.search(err[\"pattern\"], sentence)\n",
        "            if found:\n",
        "\n",
        "                # error \"di\" dan \"ke\"\n",
        "                if (err[\"err_id\"] == 1 or err[\"err_id\"] == 2) and \\\n",
        "                (testing[id] not in err[\"pengecualian\"]):\n",
        "                    kata = testing[id]\n",
        "                    k1 = kata[:2]  # di\n",
        "                    k2 = kata[2:]  # rumah\n",
        "                    koreksi = k1, k2\n",
        "                    testing[id:id+1] = (koreksi)\n",
        "                    grammar_id.extend([id, id+1])\n",
        "                \n",
        "                # error \"pun\"\n",
        "                if err[\"err_id\"] == 3 and \\\n",
        "                (testing[id] not in err[\"pengecualian\"]):\n",
        "                    kata = testing[id]\n",
        "                    k1 = kata[:-3]  # makanan\n",
        "                    k2 = kata[-3:]  # pun\n",
        "                    koreksi = k1, k2\n",
        "                    testing[id:id+1] = (koreksi)\n",
        "                    grammar_id.append(id)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_E2nrmLBhocx"
      },
      "source": [
        "## Setelah POS tag"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaCxpAqhcIEo"
      },
      "source": [
        "Sekarang input kalimat yang sudah diedit ke model POS tagger"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQfvNqangBWs"
      },
      "source": [
        "# grammar rules\n",
        "grammar_pos = [\n",
        "\t{\n",
        "\t\t\"pattern\":[\"VB\", \"PRP\", \"NN\"],\n",
        "\t\t\"pesan\":\"Struktur kata kerja + saya/mereka/dia + kata benda tidak sesuai.\",\n",
        "        \"koreksi\":[0,2,1],  # VB NN PRP\n",
        "        \"contoh\":\"memakan apel saya\",\n",
        "        \"status\":\"error1\",\n",
        "        \"pengecualian\":[\"memberi\"],\n",
        "        \"err_id\":4\n",
        "\t},    \n",
        "    {\n",
        "\t\t\"pattern\":[\"JJ\", \"NN\"],\n",
        "\t\t\"pesan\":\"Kata sifat ditempatkan setelah kata benda.\",\n",
        "        \"koreksi\":[1, 0],  # NN JJ\n",
        "        \"contoh\":\"mobil merah\",\n",
        "        \"status\":\"error1\",\n",
        "        \"pengecualian\":[\"besok\", \"hari\", \"kemarin\", \"pagi\", \"saat\"],\n",
        "        \"err_id\":7\n",
        "\t},\n",
        "    {\n",
        "\t\t\"pattern\":[\"IN\", \"VB\"],\n",
        "\t\t\"pesan\":\"'di' harus digabung dengan kata kerja.\",\n",
        "        \"koreksi\":\"VB\",  # replace \"IN\", \"VB\" with \"VB\"\n",
        "        \"contoh\":\"dimakan\",\n",
        "        \"status\":\"error1\",\n",
        "        \"pengecualian\":[],\n",
        "        \"err_id\":5\n",
        "\t},\n",
        "    {\n",
        "\t\t\"pattern\":[\"PRP\", \"NN\"],\n",
        "\t\t\"pesan\":\"Struktur saya/mereka/dia diikuti kata benda tidak sesuai\",\n",
        "        \"koreksi\":[1, 0],  # NN JJ\n",
        "        \"contoh\":\"meja saya\",\n",
        "        \"status\":\"error1\",\n",
        "        \"pengecualian\":[\"memberi\"],\n",
        "        \"err_id\":8\n",
        "\t},\n",
        "    {\n",
        "\t\t\"pattern\":[\"PRP\", \"VB\", \"WH\"],\n",
        "\t\t\"pesan\":\"Kalimat tanya diakhiri dengan tanda tanya\",\n",
        "        \"koreksi\":\"Z\",  # add \"?\"\n",
        "        \"contoh\":\"kamu makan apa?\",\n",
        "        \"status\":\"error3\",\n",
        "        \"pengecualian\":[],\n",
        "        \"err_id\":16\n",
        "\t},\n",
        "    {\n",
        "\t\t\"pattern\":[\"VB\"],\n",
        "\t\t\"pesan\":\"Kata sangat atau agak tidak bisa digabung dengan kata kerja.\",\n",
        "        \"koreksi\":\"VB\",  # replace \"sangat/agak\", \"VB\" with \"VB\"\n",
        "        \"contoh\":\"sangat makan\",\n",
        "        \"status\":\"error1\",\n",
        "        \"pengecualian\":[\"membahayakan\", \"berbahaya\", \"berbisa\", \"kecewa\", \\\n",
        "                        \"mengecewakan\", \"suka\", \"menyukai\", \"berharap\", \\\n",
        "                        \"mengharapkan\", \"membenci\", \"berjasa\"],\n",
        "        \"err_id\":6\n",
        "\t},\n",
        "    {\n",
        "\t\t\"pattern\":[\"VB\", \"NEG\"],\n",
        "\t\t\"pesan\":\"Struktur yang benar adalah kata negasi+kata kerja\",\n",
        "        \"koreksi\":[1, 0],  # NEG VB\n",
        "        \"contoh\":\"tidak makan\",\n",
        "        \"status\":\"error1\",\n",
        "        \"pengecualian\":[],\n",
        "        \"err_id\":9\n",
        "\t},\n",
        "    {\n",
        "\t\t\"pattern\":[\"VB\", \"RB\"],\n",
        "\t\t\"pesan\":\"Kata kerja tidak bisa digabung dengan kata 'sekali'.\",\n",
        "        \"koreksi\":\"VB\",  # replace \"VB\", \"sekali\" with \"VB\"\n",
        "        \"contoh\":\"makan sekali\",\n",
        "        \"status\":\"error1\",\n",
        "        \"pengecualian\":[\"membahayakan\", \"berbahaya\", \"berbisa\", \"kecewa\", \\\n",
        "                        \"mengecewakan\", \"suka\", \"menyukai\", \"berharap\", \\\n",
        "                        \"mengharapkan\", \"membenci\", \"berjasa\"],\n",
        "        \"err_id\":10\n",
        "\t},\n",
        "    {\n",
        "\t\t\"pattern\":[\"RB\", \"JJ\", \"RB\"],\n",
        "\t\t\"pesan\":\"Struktur kata sangat+kata sifat+sekali tidak efektif.\",\n",
        "        \"koreksi\":\"\",  # remove \"sekali\"\n",
        "        \"contoh\":\"sangat indah sekali\",\n",
        "        \"status\":\"error2\",\n",
        "        \"pengecualian\":[],\n",
        "        \"err_id\":11\n",
        "\t},\n",
        "    {\n",
        "\t\t\"pattern\":[\"SC\", \"SC\"],\n",
        "\t\t\"pesan\":\"Struktur kata agar+supaya tidak efektif\",\n",
        "        \"koreksi\":\"\",  # remove \"agar\"\n",
        "        \"contoh\":\"Andi belajar agar lulus\",\n",
        "        \"status\":\"error2\",\n",
        "        \"pengecualian\":[],\n",
        "        \"err_id\":12\n",
        "\t},\n",
        "    {\n",
        "\t\t\"pattern\":[\"IN\", \"SC\"],\n",
        "\t\t\"pesan\":\"Struktur kata demi+untuk tidak efektif\",\n",
        "        \"koreksi\":\"\",  # remove \"demi\"\n",
        "        \"contoh\":\"dia bekerja untuk kebutuhan hidup\",\n",
        "        \"status\":\"error2\",\n",
        "        \"pengecualian\":[],\n",
        "        \"err_id\":13\n",
        "\t},\n",
        "    {\n",
        "\t\t\"pattern\":[\"CD\", \"VB\", \"CD\"],\n",
        "\t\t\"pesan\":\"Struktur kata banyak+terdapat+berbagai tidak efektif\",\n",
        "        \"koreksi\":\"\",  # remove \"banyak\"\n",
        "        \"contoh\":\"terdapat berbagai jenis tanaman\",\n",
        "        \"status\":\"error2\",\n",
        "        \"pengecualian\":[],\n",
        "        \"err_id\":14\n",
        "\t},\n",
        "    {\n",
        "\t\t\"pattern\":[\"DT\", \"NN\", \"CD\"],\n",
        "\t\t\"pesan\":\"Struktur kata para+kata benda+semua tidak efektif\",\n",
        "        \"koreksi\":\"\",  # remove \"para\"\n",
        "        \"contoh\":\"para hadirin semua\",\n",
        "        \"status\":\"error2\",\n",
        "        \"pengecualian\":[],\n",
        "        \"err_id\":15\n",
        "\t},\n",
        "    {\n",
        "\t\t\"pattern\":[\"VB\", \"RP\"],\n",
        "\t\t\"pesan\":\"Kata kerja digabung dengan partikel 'lah'\",\n",
        "        \"koreksi\":\"VB\",  # replace \"IN\", \"VB\" with \"VB\"\n",
        "        \"contoh\":\"makanlah\",\n",
        "        \"status\":\"error1\",\n",
        "        \"pengecualian\":[],\n",
        "        \"err_id\":17\n",
        "\t},\n",
        "    {\n",
        "\t\t\"pattern\":[\"NNP\"],\n",
        "\t\t\"pesan\":\"Gunakan huruf besar diawal kata\",\n",
        "        \"koreksi\":\"\",  # capitalize the first letter\n",
        "        \"contoh\":\"Budi\",\n",
        "        \"status\":\"error4\",\n",
        "        \"pengecualian\":[],\n",
        "        \"err_id\":18\n",
        "\t},\n",
        "    {\n",
        "\t\t\"pattern\":[\"WH\"],\n",
        "\t\t\"pesan\":\"Kalimat tanya diakhiri dengan tanda tanya\",\n",
        "        \"koreksi\":\"\",  # add \"?\" or change \".\" to \"?\"\n",
        "        \"contoh\":\"siapa kalian?\",\n",
        "        \"status\":\"error3\",\n",
        "        \"pengecualian\":[],\n",
        "        \"err_id\":19\n",
        "\t},\n",
        "    {\n",
        "\t\t\"pattern\":[\"SC\"],\n",
        "\t\t\"pesan\":\"Kalimat tanya diakhiri dengan tanda tanya\",\n",
        "        \"koreksi\":\"\",  # add \"?\" or change \".\" to \"?\"\n",
        "        \"contoh\":\"siapa kalian?\",\n",
        "        \"status\":\"error3\",\n",
        "        \"pengecualian\":[],\n",
        "        \"err_id\":19\n",
        "\t},\n",
        "    {\n",
        "\t\t\"pattern\":[\"SYM\", \"CD\"],\n",
        "\t\t\"pesan\":\"Nilai mata uang disambung dengan nominal\",\n",
        "        \"koreksi\":\"CD\",  # \n",
        "        \"contoh\":\"Rp100\",\n",
        "        \"status\":\"error4\",\n",
        "        \"pengecualian\":[],\n",
        "        \"err_id\":20\n",
        "\t},\n",
        "    {\n",
        "\t\t\"pattern\":[\"NN\"],\n",
        "\t\t\"pesan\":\"'jam' diganti dengan 'pukul'\",\n",
        "        \"koreksi\":\"\",  # \n",
        "        \"contoh\":\"pukul 6 sore\",\n",
        "        \"status\":\"error4\",\n",
        "        \"pengecualian\":[],\n",
        "        \"err_id\":21\n",
        "\t},\n",
        "    {\n",
        "\t\t\"pattern\":[\"IN\"],\n",
        "\t\t\"pesan\":\"Struktur kata mundur+ke+belakang tidak efektif\",\n",
        "        \"koreksi\":\"\",  # remove \"ke belakang\"\n",
        "        \"contoh\":\"dia mundur ke belakang\",\n",
        "        \"status\":\"error2\",\n",
        "        \"pengecualian\":[],\n",
        "        \"err_id\":22\n",
        "\t},\n",
        "    {\n",
        "\t\t\"pattern\":[\"CC\"],\n",
        "\t\t\"pesan\":\"Gunakan tanda koma sebelum kata 'dan'\",\n",
        "        \"koreksi\":\"\",  # ..., ..., dan ...\n",
        "        \"contoh\":\"ayah, ibu, dan adik\",\n",
        "        \"status\":\"error3\",\n",
        "        \"pengecualian\":[],\n",
        "        \"err_id\":23\n",
        "\t},\n",
        "]"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euV5jNtWMCTd"
      },
      "source": [
        "def grammar_checker(testing, testing_list, testing_pos):\n",
        "\n",
        "    check = 0\n",
        "\n",
        "    # iterate through possible errors\n",
        "    for err in grammar_pos:\n",
        "        indexes = []\n",
        "\n",
        "        # look for error patterns in the list of POS tags\n",
        "        for i in range(len(testing_pos)):\n",
        "\n",
        "            # i += check\n",
        "\n",
        "            if testing_pos[i:i+len(err[\"pattern\"])] == err[\"pattern\"]:  # found a match\n",
        "                indexes.append((i, i+len(err[\"pattern\"])))\n",
        "\n",
        "                # error VB PRP NN\n",
        "                if err[\"err_id\"] == 4 and \\\n",
        "                (testing_list[i] not in err[\"pengecualian\"]):\n",
        "                    koreksi_pos = [testing_pos[i:i+len(err[\"pattern\"])][a] for a in err[\"koreksi\"]]\n",
        "                    koreksi = [testing_list[i:i+len(err[\"pattern\"])][a] for a in err[\"koreksi\"]]\n",
        "\n",
        "                    for j in range(len(indexes)):\n",
        "                        testing_list[i:indexes[j][1]] = koreksi\n",
        "                        testing_pos[indexes[j][0]:indexes[j][1]] = koreksi_pos\n",
        "                    \n",
        "                    grammar_id.extend(range(i, i+len(err[\"pattern\"])))\n",
        "\n",
        "                # error \"di\" + kata kerja\n",
        "                if err[\"err_id\"] == 5 and testing_list[i] == \"di\":\n",
        "                    \n",
        "                    for j in range(len(indexes)):\n",
        "                        k1 = testing_list[indexes[j][0]]\n",
        "                        k2 = testing_list[indexes[j][0]+1]\n",
        "                        testing_list[indexes[j][0]] = k1 + k2\n",
        "                        testing_list.pop(indexes[j][0]+1)\n",
        "                        testing_pos.pop(i)\n",
        "                        grammar_id.extend([indexes[j][0], indexes[j][0]+1])\n",
        "                        # check += 1\n",
        "\n",
        "                # error \"sangat\"/\"agak\" + kata kerja\n",
        "                if err[\"err_id\"] == 6 and (i != 0):\n",
        "\n",
        "                    if (testing_list[i-1] == \"sangat\" or testing_list[i-1] == \"agak\") \\\n",
        "                    and (testing_list[i] not in err[\"pengecualian\"]):\n",
        "                        testing_list.pop(i-1)\n",
        "                        testing_pos.pop(i-1)\n",
        "                        grammar_id.extend([i-1, i])\n",
        "                        # check -= 1\n",
        "\n",
        "\n",
        "                    # error PRP/NN + VB + \"sedang\"\n",
        "                    if i != len(testing_list)-1:\n",
        "                        if ((testing_pos[i-1] == \"PRP\") or (testing_pos[i-1] == \"NN\")) and \\\n",
        "                        (testing_list[i+1] == \"sedang\"):\n",
        "                            testing_list[i], testing_list[i+1] = testing_list[i+1], testing_list[i]\n",
        "                            testing_pos[i], testing_pos[i+1] = testing_pos[i+1], testing_pos[i]\n",
        "                            grammar_id.extend([i-1, i, i+1])\n",
        "\n",
        "                # error JJ NN\n",
        "                if err[\"err_id\"] == 7 and \\\n",
        "                (testing_list[i+1] not in err[\"pengecualian\"]):\n",
        "                    koreksi_pos = [testing_pos[i:i+len(err[\"pattern\"])][a] for a in err[\"koreksi\"]]\n",
        "                    koreksi = [testing_list[i:i+len(err[\"pattern\"])][a] for a in err[\"koreksi\"]]\n",
        "\n",
        "                    for j in range(len(indexes)):\n",
        "                        testing_list[i:indexes[j][1]] = koreksi\n",
        "                        testing_pos[indexes[j][0]:indexes[j][1]] = koreksi_pos\n",
        "                        grammar_id.extend([i, i+1])\n",
        "\n",
        "                # error PRP NN\n",
        "                if err[\"err_id\"] == 8:\n",
        "                    if i != 0:\n",
        "                        if testing_pos[i-1] == \"VB\":\n",
        "                            continue\n",
        "\n",
        "                    koreksi_pos = [testing_pos[i:i+len(err[\"pattern\"])][a] for a in err[\"koreksi\"]]\n",
        "                    koreksi = [testing_list[i:i+len(err[\"pattern\"])][a] for a in err[\"koreksi\"]]\n",
        "\n",
        "                    for j in range(len(indexes)):\n",
        "                        testing_list[i:indexes[j][1]] = koreksi\n",
        "                        testing_pos[indexes[j][0]:indexes[j][1]] = koreksi_pos\n",
        "                        grammar_id.extend([i, i+1])\n",
        "\n",
        "                # error VB NEG\n",
        "                if err[\"err_id\"] == 9:\n",
        "                    koreksi_pos = [testing_pos[i:i+len(err[\"pattern\"])][a] for a in err[\"koreksi\"]]\n",
        "                    koreksi = [testing_list[i:i+len(err[\"pattern\"])][a] for a in err[\"koreksi\"]]\n",
        "\n",
        "                    for j in range(len(indexes)):\n",
        "                        testing_list[i:indexes[j][1]] = koreksi\n",
        "                        testing_pos[indexes[j][0]:indexes[j][1]] = koreksi_pos\n",
        "                        grammar_id.extend([i, i+1])\n",
        "\n",
        "                # error kata kerja + \"sekali\"\n",
        "                if err[\"err_id\"] == 10 and (testing_list[i+1] == \"sekali\") and \\\n",
        "                (testing_list[i] not in err[\"pengecualian\"]):\n",
        "                    testing_list.pop(i+1)\n",
        "                    testing_pos.pop(i+1)\n",
        "                    grammar_id.extend([i, i+1])\n",
        "                    # check -= 1\n",
        "                \n",
        "                # error \"sangat\" + kata sifat + \"sekali\"\n",
        "                if err[\"err_id\"] == 11 and (testing_list[i+2] == \"sekali\") and \\\n",
        "                (testing_list[i] == \"sangat\"):\n",
        "                    testing_list.pop(i+2)\n",
        "                    testing_pos.pop(i+2)\n",
        "                    efektif_id.extend([i, i+2, i+3])\n",
        "                \n",
        "                # error \"agar\" + \"supaya\"\n",
        "                if err[\"err_id\"] == 12 and (testing_list[i] == \"agar\") and \\\n",
        "                (testing_list[i+1] == \"supaya\"):\n",
        "                    testing_list.pop(i+1)\n",
        "                    testing_pos.pop(i+1)\n",
        "                    efektif_id.extend([i, i+1])\n",
        "\n",
        "                # error \"demi\" + \"untuk\"\n",
        "                if err[\"err_id\"] == 13 and (testing_list[i] == \"demi\") and \\\n",
        "                (testing_list[i+1] == \"untuk\"):\n",
        "                    testing_list.pop(i)\n",
        "                    testing_pos.pop(i)\n",
        "                    efektif_id.extend([i, i+1])\n",
        "                \n",
        "                # error \"banyak\" + \"terdapat\" + \"berbagai\"\n",
        "                if err[\"err_id\"] == 14 and (testing_list[i] == \"banyak\") and \\\n",
        "                (testing_list[i+1] == \"terdapat\") and \\\n",
        "                (testing_list[i+2] == \"berbagai\"):\n",
        "                    testing_list.pop(i)\n",
        "                    testing_pos.pop(i)\n",
        "                    efektif_id.extend(range(i, i+3))\n",
        "\n",
        "                # error \"para\" + kata benda + \"semua\"\n",
        "                if err[\"err_id\"] == 15 and (testing_list[i] == \"para\") and \\\n",
        "                (testing_list[i+2] == \"semua\"):\n",
        "                    testing_list.pop(i)\n",
        "                    testing_pos.pop(i)\n",
        "                    efektif_id.extend([i, i+1])\n",
        "                \n",
        "                # error tanda tanya\n",
        "                if err[\"err_id\"] == 16:\n",
        "                    # end of sentence\n",
        "                    if i+len(err[\"pattern\"]) >= len(testing_list):\n",
        "                        testing_list.append(\"?\")\n",
        "                        testing_pos.append(\"Z\")\n",
        "                        punc_id.extend(range(i, i+len(err[\"pattern\"])))\n",
        "\n",
        "                    elif testing_list[i+3] != \"?\":\n",
        "                        testing_list.insert(i+len(err[\"pattern\"]), \"?\")\n",
        "                        testing_pos.insert(i+len(err[\"pattern\"]), \"Z\")\n",
        "                        punc_id.extend(range(i, i+len(err[\"pattern\"])))\n",
        "\n",
        "                # error kata kerja + \"lah\"\n",
        "                if err[\"err_id\"] == 17 and testing_list[i+1] == \"lah\":\n",
        "                    \n",
        "                    for j in range(len(indexes)):\n",
        "                        k1 = testing_list[indexes[j][0]]\n",
        "                        k2 = testing_list[indexes[j][0]+1]\n",
        "                        testing_list[indexes[j][0]] = k1 + k2\n",
        "                        testing_list.pop(indexes[j][0]+1)\n",
        "                        testing_pos.pop(i)\n",
        "                        grammar_id.extend([i, i+1])\n",
        "\n",
        "                # error huruf kapital\n",
        "                if err[\"err_id\"] == 18:\n",
        "                    vowels = ['a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U']\n",
        "                    if testing_list[i] != testing_list[i].capitalize():\n",
        "                        testing_list[i] = testing_list[i].capitalize()\n",
        "                        if (len([j for j in testing_list[i] if j in vowels]) == 0) \\\n",
        "                        or (len([j for j in testing_list[i] if j in vowels]) == len(testing_list[i])):\n",
        "                            testing_list[i] = testing_list[i].upper()\n",
        "                        kapital_id.append(i)\n",
        "                \n",
        "                # error tanda tanya part 2\n",
        "                if err[\"err_id\"] == 19 and ((i == 0) or (testing_pos[i-1] == \"Z\")):\n",
        "\n",
        "                    if (testing_pos[i] == \"WH\") or ((testing_pos[i] == \"SC\") \\\n",
        "                    and (testing_list[i] in [\"siapa\", \"bagaimana\", \"apa\", \\\n",
        "                                          \"kapan\", \"di mana\", \"berapa\", \\\n",
        "                                          \"mengapa\", \"mengapa\"])):\n",
        "                        z_id = [j for j, x in enumerate(testing_pos[i+1:]) if x == \"Z\"]\n",
        "                        # no punctuation after the question\n",
        "                        if not z_id:\n",
        "                            testing_list.append(\"?\")\n",
        "                            testing_pos.append(\"Z\")\n",
        "                            punc_id.append(len(kalimat)-1)\n",
        "                        \n",
        "                        # punctuation isn't a question mark\n",
        "                        else:\n",
        "                            # if testing_list[i+z_id[0]+1] != \"?\":\n",
        "                            #     if testing_list[i+z_id[1]+1] in [\"'\", '\"', \")\", \"]\", \"}\"]:\n",
        "                            #         k1 = \"?\"\n",
        "                            #         k2 = \n",
        "                            #         testing_\n",
        "                            #     else:\n",
        "                            #         testing_list[i+z_id[0]+1] = \"?\"\n",
        "                            #         testing_pos[i+z_id[0]+1] = \"Z\"\n",
        "                            #     punc_id.append(i+z_id[0]+1)\n",
        "                          \n",
        "                            if testing_list[i+z_id[0]+1] != \"?\":\n",
        "                                if testing_list[i+z_id[0]+1] in [\"'\", '\"', \")\", \"]\", \"}\"]:\n",
        "                                    testing_list.insert(i+z_id[0]+1, \"?\")\n",
        "                                    testing_pos.insert(i+z_id[0]+1, \"Z\")\n",
        "                                else:\n",
        "                                    testing_list[i+z_id[0]+1] = \"?\"\n",
        "                                    testing_pos[i+z_id[0]+1] = \"Z\"\n",
        "                                punc_id.append(i+z_id[0]+1)\n",
        "\n",
        "                # error \"Rp\" + nominal\n",
        "                if err[\"err_id\"] == 20 and testing_list[i] in [\"Rp\", \"rp\", \"RP\"]:\n",
        "                    \n",
        "                    for j in range(len(indexes)):\n",
        "                        k1 = testing_list[indexes[j][0]]\n",
        "                        k2 = testing_list[indexes[j][0]+1]\n",
        "                        testing_list[indexes[j][0]] = k1 + k2\n",
        "                        testing_list.pop(indexes[j][0]+1)\n",
        "                        testing_pos.pop(i)\n",
        "                        grammar_id.extend([i, i+1])\n",
        "                \n",
        "                # error \"pukul\" + angka\n",
        "                if err[\"err_id\"] == 21 and testing_list[i] in [\"jam\", \"pukul\"] \\\n",
        "                and testing_list[i+1].isnumeric() and i != len(testing_list):\n",
        "\n",
        "                    if testing_list[i] == \"jam\":\n",
        "                        testing_list[i] = \"pukul\"\n",
        "                        grammar_id.append(i)\n",
        "\n",
        "                    if i+1 != len(testing_list)-1:\n",
        "                        z_id = [j for j, x in enumerate(testing_pos[i+1:]) if x == \"Z\"]\n",
        "                        # error \":\" \n",
        "                        if (z_id[0] == 1):\n",
        "                            if (testing_list[i+2] == \":\") and \\\n",
        "                            (testing_list[i+3].isnumeric()):\n",
        "                                k1 = testing_list[i+1]\n",
        "                                k2 = \".\"\n",
        "                                k3 = testing_list[i+3]\n",
        "                                testing_list[i+1] = k1 + k2 + k3\n",
        "                                testing_list.pop(i+2)\n",
        "                                testing_list.pop(i+2)\n",
        "                                testing_pos.pop(i+2)\n",
        "                                testing_pos.pop(i+2)\n",
        "                                grammar_id.extend(range(i+1, i+4))\n",
        "                \n",
        "                # error \"maju/mundur/naik\" + \"ke\" + \"depan/belakang/atas\"\n",
        "                if err[\"err_id\"] == 22 and i != 0 and i != len(testing_list)-1:\n",
        "                    if (testing_list[i-1] in [\"naik\", \"mundur\", \"maju\"]) and \\\n",
        "                    (testing_list[i] == \"ke\") and \\\n",
        "                    (testing_list[i+1] in [\"atas\", \"belakang\", \"depan\"]):\n",
        "                        testing_list.pop(i)\n",
        "                        testing_pos.pop(i)\n",
        "                        testing_list.pop(i)\n",
        "                        testing_pos.pop(i)\n",
        "                        efektif_id.extend(range(i-1, i+2))\n",
        "\n",
        "                # error koma sebelum \"dan\"\n",
        "                if err[\"err_id\"] == 23 and testing_list[i] == \"dan\" and i-3 >= 0:\n",
        "                    if testing_list[i-2] == \",\":\n",
        "                        testing_list.insert(i, \",\")\n",
        "                        testing_pos.insert(i, \"Z\")\n",
        "                        punc_id.append(i)\n",
        "                        \n",
        "\n",
        "                testing = \" \".join(testing_list)\n",
        "\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCseCeBMrVwL"
      },
      "source": [
        "def finishing_touch(testing_list, testing_pos, kapital_id):\n",
        "    # capitalize the first word\n",
        "    if testing_list[0] != testing_list[0].capitalize():\n",
        "        kapital_id.append(0)\n",
        "    testing_list[0] = testing_list[0].capitalize()\n",
        "\n",
        "    # end the sentence with a dot\n",
        "    if (testing_list[-1] != \".\") and (testing_list[-1] != \"?\"):\n",
        "        testing_list.append(\".\")\n",
        "        testing_pos.append(\"Z\")\n",
        "        punc_id.append(len(kalimat)-1)\n",
        "\n",
        "    z_id = [i for i, x in enumerate(testing_pos) if x == \"Z\"]\n",
        "\n",
        "    # capitalize the first word after a dot or a question mark\n",
        "    for i in z_id[:-1]:\n",
        "        if testing_list[i] in [\".\", \"?\", \"!\", \"'\", '\"']:\n",
        "            testing_list[i+1] = testing_list[i+1].capitalize()\n",
        "\n",
        "    # get kapital_id\n",
        "    z_id_1 = [i for i, x in enumerate(kalimat[:-1]) if x in [\".\", \"?\", \"!\", \"'\", '\"']]\n",
        "    for i in z_id_1:\n",
        "        if kalimat[i+1] != kalimat[i+1].capitalize():\n",
        "                    kapital_id.append(i+1)\n",
        "\n",
        "    check = 0\n",
        "    comma = 0\n",
        "\n",
        "    for i in z_id:\n",
        "\n",
        "        sym_count = 0\n",
        "        # count previous quotation marks\n",
        "        for kata in testing_list[:i-3]:\n",
        "            for a in kata:\n",
        "                if a in [\"'\", '\"']:\n",
        "                    sym_count += 1\n",
        "\n",
        "        i += check\n",
        "\n",
        "        try:\n",
        "            if i != len(testing_list)-1 and i < len(testing_list):\n",
        "\n",
        "                # merge \" with the next word\n",
        "                if (sym_count % 2 == 0) and \\\n",
        "                (testing_list[i] in [\"'\", '\"']):\n",
        "                    k1 = testing_list[i]\n",
        "                    k2 = testing_list[i+1]\n",
        "                    testing_list[i] = k1 + k2\n",
        "                    testing_list.pop(i+1)\n",
        "                    testing_pos.pop(i)\n",
        "                    check -= 1\n",
        "                \n",
        "                    # no punctuation before \"\"                \n",
        "                    if (testing_pos[i-1] != \"Z\") and \\\n",
        "                    (testing_list[i-1][-1] not in [\",\", \".\", \"?\", \"!\"]):\n",
        "                        testing_list.insert(i, \",\")\n",
        "                        k1 = testing_list[i-1]\n",
        "                        k2 = testing_list[i]\n",
        "                        testing_list[i-1] = k1 + k2\n",
        "                        testing_list.pop(i)\n",
        "                        testing_pos.pop(i)\n",
        "                        check -= 1\n",
        "\n",
        "                # merge \" with the previous word (comma before \"\")\n",
        "                elif (testing_list[i] in [\",\", \".\", \"?\", \"!\"]) and \\\n",
        "                (testing_list[i+1] in [\"'\", '\"']):\n",
        "                    k1 = testing_list[i-1]\n",
        "                    k2 = testing_list[i]\n",
        "                    testing_list[i-1] = k1 + k2\n",
        "                    testing_list.pop(i)\n",
        "                    testing_pos.pop(i)\n",
        "                    check -= 1\n",
        "\n",
        "                # merge \" with the previous word (normal sentence)\n",
        "                elif i != 0 and (testing_list[i] not in [\"-\", \"(\"]):\n",
        "                    k1 = testing_list[i-1]\n",
        "                    k2 = testing_list[i]\n",
        "                    testing_list[i-1] = k1 + k2\n",
        "                    testing_list.pop(i)\n",
        "                    testing_pos.pop(i)\n",
        "                    check -= 1\n",
        "\n",
        "                # brackets/parentheses\n",
        "                elif testing_list[i] in [\"(\"]:\n",
        "                    k1 = testing_list[i]\n",
        "                    k2 = testing_list[i+1]\n",
        "                    testing_list[i] = k1 + k2\n",
        "                    testing_list.pop(i+1)\n",
        "                    testing_pos.pop(i)\n",
        "                    check -= 1\n",
        "\n",
        "                # plural nouns with \"-\"\n",
        "                if (i != 0) and (i != len(testing_list)-1) and \\\n",
        "                (testing_list[i] == \"-\"):\n",
        "                    k1 = testing_list[i-1]\n",
        "                    k2 = testing_list[i]\n",
        "                    k3 = testing_list[i+1]\n",
        "                    testing_list[i-1] = k1 + k2 + k3\n",
        "                    testing_list.pop(i)\n",
        "                    testing_pos.pop(i)\n",
        "                    testing_list.pop(i)\n",
        "                    testing_pos.pop(i)\n",
        "                    check -= 2\n",
        "            \n",
        "            else:\n",
        "                k1 = testing_list[i-1]\n",
        "                k2 = testing_list[i]\n",
        "                testing_list[i-1] = k1 + k2\n",
        "                testing_list.pop(i)\n",
        "                testing_pos.pop(i)\n",
        "                check -= 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "\n",
        "    return \" \".join(testing_list), kapital_id"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaAftjIHrkh7"
      },
      "source": [
        "# EsAI.id"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMf-_cOVBvgr",
        "outputId": "8131c77f-585e-4ccf-f602-28d15c5b97d7"
      },
      "source": [
        "kalimat = \"dia beertanya, 'anda makan apa' Dia sedang sangat makn saya apel dikantor. saya naik ke atas\"\n",
        "print(f\"Original text:  {kalimat}\")\n",
        "print(\"=\"*150)\n",
        "\n",
        "punc_id = []\n",
        "efektif_id = []\n",
        "grammar_id = []\n",
        "kapital_id = []\n",
        "\n",
        "# preprocessing\n",
        "kalimat = split_kalimat(kalimat)\n",
        "\n",
        "# typo checker\n",
        "kalimat_baru = []\n",
        "typos_id = []\n",
        "for id, kata in enumerate(kalimat):\n",
        "    if (kata.isalpha()) and ((kata != \"Rp\") or (kata.isnumeric())):\n",
        "        kalimat_baru.append(correction(kata.lower()))\n",
        "        if correction(kata).lower() != kata.lower():\n",
        "            typos_id.append(id)\n",
        "    else:\n",
        "        kalimat_baru.append(kata)\n",
        "print(f\"1   Typo correction:    {kalimat_baru}\")\n",
        "\n",
        "# rule-based word/typo checker\n",
        "word_checker(kalimat_baru)\n",
        "print(f\"2   Word checker:       {kalimat_baru}\")\n",
        "\n",
        "# pos tagger\n",
        "pos_kalimat = prediksi_pos(kalimat_baru)\n",
        "print(f\"3   POS tags:           {pos_kalimat}\")\n",
        "\n",
        "# rule-based grammar checker\n",
        "grammar_checker(\" \".join(kalimat_baru), kalimat_baru, pos_kalimat)\n",
        "print(f\"4   Grammar checker:    {kalimat_baru}\")\n",
        "\n",
        "# capitalization + finishing touches\n",
        "kalimat_final, kapital_id = finishing_touch(kalimat_baru, pos_kalimat, kapital_id)\n",
        "\n",
        "# typos etc.\n",
        "print(\"=\"*150)\n",
        "print(\"Detected errors:\")\n",
        "print(f\"1   Typos:              {sorted(typos_id)}\")\n",
        "print(f\"2   Huruf kapital:      {sorted(kapital_id)}\")\n",
        "print(f\"3   Tanda baca:         {sorted(punc_id)}\")\n",
        "print(f\"4   Tidak efektif:      {sorted(efektif_id)}\")\n",
        "print(f\"5   Grammar/lainnya:    {sorted(set(grammar_id))}\")\n",
        "\n",
        "print(\"=\"*150)\n",
        "print(f\"Corrected text: {kalimat_final}\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text:  dia beertanya, 'anda makan apa' Dia sedang sangat makn saya apel dikantor. saya naik ke atas\n",
            "======================================================================================================================================================\n",
            "1   Typo correction:    ['dia', 'bertanya', ',', \"'\", 'anda', 'makan', 'apa', \"'\", 'dia', 'sedang', 'sangat', 'makan', 'saya', 'apel', 'dikantor', '.', 'saya', 'naik', 'ke', 'atas']\n",
            "2   Word checker:       ['dia', 'bertanya', ',', \"'\", 'anda', 'makan', 'apa', \"'\", 'dia', 'sedang', 'sangat', 'makan', 'saya', 'apel', 'di', 'kantor', '.', 'saya', 'naik', 'ke', 'atas']\n",
            "3   POS tags:           ['PRP', 'VB', 'Z', 'Z', 'PRP', 'VB', 'WH', 'Z', 'PRP', 'MD', 'RB', 'VB', 'PRP', 'NN', 'IN', 'NN', 'Z', 'PRP', 'VB', 'IN', 'IN']\n",
            "4   Grammar checker:    ['dia', 'bertanya', ',', \"'\", 'anda', 'makan', 'apa', '?', \"'\", 'dia', 'sedang', 'makan', 'apel', 'saya', 'di', 'kantor', '.', 'saya', 'naik']\n",
            "======================================================================================================================================================\n",
            "Detected errors:\n",
            "1   Typos:              [1, 11]\n",
            "2   Huruf kapital:      [0, 4, 16]\n",
            "3   Tanda baca:         [4, 5, 6, 19]\n",
            "4   Tidak efektif:      [18, 19, 20]\n",
            "5   Grammar/lainnya:    [11, 12, 13, 14, 15]\n",
            "======================================================================================================================================================\n",
            "Corrected text: Dia bertanya, 'Anda makan apa?' Dia sedang makan apel saya di kantor. Saya naik.\n"
          ]
        }
      ]
    }
  ]
}